{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune PhoBERT cho Fact Checking\n",
    "Dataset: ise-dsc01 (Vietnamese Fact Verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate -q\n",
    "!pip install sentencepiece -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. C·∫•u h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    MODEL_NAME = \"vinai/phobert-base\"\n",
    "    MAX_LENGTH = 256\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 5\n",
    "    WARMUP_STEPS = 500\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n dataset c·ªßa b·∫°n\n",
    "    TRAIN_PATH = \"/kaggle/input/your-dataset/ise-dsc01-train.json\"\n",
    "    WARMUP_PATH = \"/kaggle/input/your-dataset/ise-dsc01-warmup.json\"\n",
    "    \n",
    "    LABEL2ID = {\"SUPPORTED\": 0, \"REFUTED\": 1, \"NEI\": 2}\n",
    "    ID2LABEL = {0: \"SUPPORTED\", 1: \"REFUTED\", 2: \"NEI\"}\n",
    "    NUM_LABELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load v√† kh√°m ph√° d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    records = []\n",
    "    for key, value in data.items():\n",
    "        records.append({\n",
    "            'id': key,\n",
    "            'context': value['context'],\n",
    "            'claim': value['claim'],\n",
    "            'verdict': value['verdict'],\n",
    "            'evidence': value.get('evidence', ''),\n",
    "            'domain': value.get('domain', '')\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Load data\n",
    "df_train = load_data(Config.TRAIN_PATH)\n",
    "print(f\"T·ªïng s·ªë m·∫´u: {len(df_train)}\")\n",
    "print(f\"\\nPh√¢n b·ªë nh√£n:\")\n",
    "print(df_train['verdict'].value_counts())\n",
    "print(f\"\\nPh√¢n b·ªë domain:\")\n",
    "print(df_train['domain'].value_counts())\n",
    "\n",
    "# Xem m·∫´u d·ªØ li·ªáu\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chu·∫©n b·ªã Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_text(row):\n",
    "    \"\"\"K·∫øt h·ª£p claim v√† context\"\"\"\n",
    "    text = f\"Claim: {row['claim']} Context: {row['context']}\"\n",
    "    return text\n",
    "\n",
    "class FactCheckDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length, label2id):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label2id = label2id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = prepare_input_text(row)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        label = self.label2id[row['verdict']]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chia train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df_train, \n",
    "    test_size=0.15, \n",
    "    random_state=42,\n",
    "    stratify=df_train['verdict']\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")\n",
    "print(f\"\\nTrain label distribution:\")\n",
    "print(train_df['verdict'].value_counts())\n",
    "print(f\"\\nValidation label distribution:\")\n",
    "print(val_df['verdict'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    Config.MODEL_NAME,\n",
    "    num_labels=Config.NUM_LABELS,\n",
    "    id2label=Config.ID2LABEL,\n",
    "    label2id=Config.LABEL2ID\n",
    ")\n",
    "\n",
    "print(\"‚úì PhoBERT loaded successfully\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. T·∫°o datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FactCheckDataset(train_df, tokenizer, Config.MAX_LENGTH, Config.LABEL2ID)\n",
    "val_dataset = FactCheckDataset(val_df, tokenizer, Config.MAX_LENGTH, Config.LABEL2ID)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Test dataset\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Sample attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Sample label: {sample['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ƒê·ªãnh nghƒ©a metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average='macro')\n",
    "    f1_weighted = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=Config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=Config.BATCH_SIZE,\n",
    "    learning_rate=Config.LEARNING_RATE,\n",
    "    weight_decay=Config.WEIGHT_DECAY,\n",
    "    warmup_steps=Config.WARMUP_STEPS,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Kh·ªüi t·∫°o Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"B·∫Øt ƒë·∫ßu training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nüìä K·∫øt qu·∫£ tr√™n validation set:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Chi ti·∫øt classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(val_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "print(classification_report(\n",
    "    labels, \n",
    "    preds, \n",
    "    target_names=list(Config.LABEL2ID.keys())\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. L∆∞u model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./phobert_factcheck_model')\n",
    "tokenizer.save_pretrained('./phobert_factcheck_model')\n",
    "print(\"‚úì Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: ./phobert_factcheck_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_claim(claim, context, model, tokenizer, device='cuda'):\n",
    "    model.eval()\n",
    "    text = f\"Claim: {claim} Context: {context}\"\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=Config.MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        pred_label = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    return Config.ID2LABEL[pred_label], probs[0].cpu().numpy()\n",
    "\n",
    "# Test v·ªõi m·ªôt m·∫´u\n",
    "test_sample = val_df.iloc[0]\n",
    "pred_label, probs = predict_claim(\n",
    "    test_sample['claim'], \n",
    "    test_sample['context'], \n",
    "    model, \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Claim: {test_sample['claim'][:100]}...\")\n",
    "print(f\"\\nTrue label: {test_sample['verdict']}\")\n",
    "print(f\"Predicted: {pred_label}\")\n",
    "print(f\"\\nProbabilities:\")\n",
    "for label, prob in zip(Config.LABEL2ID.keys(), probs):\n",
    "    print(f\"  {label}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
