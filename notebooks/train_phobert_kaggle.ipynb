{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune PhoBERT cho Claim Detection\n",
    "Binary classification: claim vs non-claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate -q\n",
    "!pip install scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import v√† ki·ªÉm tra GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. C·∫•u h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh\n",
    "MODEL_NAME = \"vinai/phobert-base\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n dataset - C·∫¨P NH·∫¨T THEO KAGGLE\n",
    "TRAIN_PATH = \"/kaggle/input/claim-detection-dataset/claim_detection_train.jsonl\"\n",
    "VAL_PATH = \"/kaggle/input/claim-detection-dataset/claim_detection_val.jsonl\"\n",
    "TEST_PATH = \"/kaggle/input/claim-detection-dataset/claim_detection_test.jsonl\"\n",
    "\n",
    "# Labels\n",
    "LABEL2ID = {\"non-claim\": 0, \"claim\": 1}\n",
    "ID2LABEL = {0: \"non-claim\", 1: \"claim\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load file JSONL\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load data\n",
    "train_df = load_jsonl(TRAIN_PATH)\n",
    "val_df = load_jsonl(VAL_PATH)\n",
    "test_df = load_jsonl(TEST_PATH)\n",
    "\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Val: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "\n",
    "print(\"\\nTrain label distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. T·∫°o Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['text']\n",
    "        label = LABEL2ID[row['label']]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model loaded\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. T·∫°o datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClaimDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = ClaimDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = ClaimDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Val dataset: {len(val_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "\n",
    "# Test\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Sample label: {sample['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ƒê·ªãnh nghƒ©a metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Kh·ªüi t·∫°o Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"B·∫ÆT ƒê·∫¶U TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ƒê√°nh gi√° tr√™n validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ƒê√ÅNH GI√Å TR√äN VALIDATION SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nK·∫øt qu·∫£:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ƒê√°nh gi√° tr√™n test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ƒê√ÅNH GI√Å TR√äN TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, target_names=['non-claim', 'claim']))\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  Accuracy: {accuracy_score(labels, preds):.4f}\")\n",
    "print(f\"  Precision: {precision_score(labels, preds):.4f}\")\n",
    "print(f\"  Recall: {recall_score(labels, preds):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(labels, preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. L∆∞u model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u model\n",
    "model.save_pretrained('./phobert_claim_detection')\n",
    "tokenizer.save_pretrained('./phobert_claim_detection')\n",
    "\n",
    "print(\"‚úì Model saved to ./phobert_claim_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    \"\"\"D·ª± ƒëo√°n m·ªôt c√¢u\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    label = ID2LABEL[pred]\n",
    "    confidence = probs[0][pred].item()\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "# Test\n",
    "test_sentences = [\n",
    "    \"Vi·ªát Nam c√≥ 54 d√¢n t·ªôc anh em\",\n",
    "    \"D√¢n s·ªë H√† N·ªôi nƒÉm 2023 ƒë·∫°t 8 tri·ªáu ng∆∞·ªùi\",\n",
    "    \"B·∫°n c√≥ bi·∫øt ƒëi·ªÅu n√†y kh√¥ng?\",\n",
    "    \"T√¥i nghƒ© r·∫±ng ƒë√¢y l√† quy·∫øt ƒë·ªãnh ƒë√∫ng ƒë·∫Øn\",\n",
    "    \"H√£y xem x√©t k·ªπ v·∫•n ƒë·ªÅ n√†y\",\n",
    "    \"TP HCM l√† th√†nh ph·ªë l·ªõn nh·∫•t Vi·ªát Nam\",\n",
    "]\n",
    "\n",
    "print(\"\\nTest predictions:\")\n",
    "print(\"=\"*60)\n",
    "for sent in test_sentences:\n",
    "    label, conf = predict(sent)\n",
    "    print(f\"\\n{sent}\")\n",
    "    print(f\"  ‚Üí {label} (confidence: {conf:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. N√©n model ƒë·ªÉ download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r phobert_claim_detection.zip phobert_claim_detection/\n",
    "print(\"\\n‚úì Model ƒë√£ ƒë∆∞·ª£c n√©n th√†nh phobert_claim_detection.zip\")\n",
    "print(\"Download t·ª´ Output tab c·ªßa Kaggle notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ho√†n th√†nh! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
