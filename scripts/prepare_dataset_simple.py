"""
Script chuáº©n bá»‹ dataset Claim Detection
TrÃ­ch xuáº¥t claims vÃ  non-claims tá»« dataset ise-dsc01

Chiáº¿n lÆ°á»£c:
1. Claims: Láº¥y tá»« field 'claim' trong dataset
2. Non-claims: TrÃ­ch xuáº¥t tá»« context (cÃ¡c cÃ¢u KHÃ”NG pháº£i claim/evidence)
3. Lá»c thÃ´ng minh Ä‘á»ƒ Ä‘áº£m báº£o cháº¥t lÆ°á»£ng
"""

import json
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split

def load_ise_dataset(file_path):
    """Load dataset ise-dsc01"""
    print(f"Loading {file_path}...")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    records = []
    for key, value in data.items():
        records.append({
            'id': key,
            'context': value['context'],
            'claim': value['claim'],
            'evidence': value.get('evidence', ''),
            'verdict': value['verdict'],
            'domain': value.get('domain', '')
        })
    
    return pd.DataFrame(records)

def split_sentences(text):
    """
    TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u
    Xá»­ lÃ½ tiáº¿ng Viá»‡t tá»‘t hÆ¡n
    """
    # ThÃªm khoáº£ng tráº¯ng sau dáº¥u cÃ¢u náº¿u thiáº¿u
    text = re.sub(r'([.!?])([A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä])', r'\1 \2', text)
    
    # TÃ¡ch cÃ¢u báº±ng dáº¥u cháº¥m, cháº¥m há»i, cháº¥m than
    # NhÆ°ng khÃ´ng tÃ¡ch náº¿u lÃ  sá»‘ tháº­p phÃ¢n hoáº·c viáº¿t táº¯t
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä"\'])', text)
    
    # Náº¿u khÃ´ng tÃ¡ch Ä‘Æ°á»£c, thá»­ cÃ¡ch khÃ¡c
    if len(sentences) <= 1:
        sentences = re.split(r'[.!?]\s+', text)
    
    # Lá»c cÃ¢u rá»—ng vÃ  quÃ¡ ngáº¯n
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
    
    return sentences

def is_question(text):
    """Kiá»ƒm tra cÃ¢u há»i"""
    text = text.strip()
    if text.endswith('?'):
        return True
    question_words = ['ai ', 'gÃ¬ ', 'nÃ o ', 'Ä‘Ã¢u ', 'sao ', 'bao giá»', 'bao nhiÃªu', 'nhÆ° tháº¿ nÃ o', 'táº¡i sao', 'vÃ¬ sao', 'cÃ³ pháº£i', 'liá»‡u ']
    return any(text.lower().startswith(w) or f' {w}' in text.lower() for w in question_words)

def is_opinion(text):
    """Kiá»ƒm tra Ã½ kiáº¿n chá»§ quan"""
    opinion_markers = [
        'tÃ´i nghÄ©', 'tÃ´i cho ráº±ng', 'theo tÃ´i', 'tÃ´i tin', 'tÃ´i cáº£m tháº¥y',
        'cÃ³ láº½', 'cÃ³ thá»ƒ', 'dÆ°á»ng nhÆ°', 'hÃ¬nh nhÆ°', 'cháº¯c lÃ ', 'cÃ³ váº»',
        'theo quan Ä‘iá»ƒm', 'theo Ã½ kiáº¿n', 'cÃ¡ nhÃ¢n tÃ´i', 'riÃªng tÃ´i'
    ]
    return any(marker in text.lower() for marker in opinion_markers)

def is_command(text):
    """Kiá»ƒm tra cÃ¢u má»‡nh lá»‡nh"""
    command_starters = ['hÃ£y ', 'Ä‘á»«ng ', 'cáº§n ', 'nÃªn ', 'pháº£i ', 'xin ', 'má»i ', 'vui lÃ²ng']
    return any(text.lower().startswith(cmd) for cmd in command_starters)

def is_connector(text):
    """Kiá»ƒm tra cÃ¢u ná»‘i/chuyá»ƒn tiáº¿p"""
    connectors = [
        'trong khi Ä‘Ã³', 'bÃªn cáº¡nh Ä‘Ã³', 'ngoÃ i ra', 'Ä‘á»“ng thá»i', 'tuy nhiÃªn',
        'máº·c dÃ¹ váº­y', 'do Ä‘Ã³', 'vÃ¬ váº­y', 'theo Ä‘Ã³', 'nhÆ° váº­y', 'tÃ³m láº¡i',
        'nÃ³i cÃ¡ch khÃ¡c', 'máº·t khÃ¡c', 'hÆ¡n ná»¯a', 'thÃªm vÃ o Ä‘Ã³'
    ]
    return any(text.lower().startswith(conn) for conn in connectors)

def has_specific_info(text):
    """
    Kiá»ƒm tra cÃ¢u cÃ³ thÃ´ng tin cá»¥ thá»ƒ (cÃ³ thá»ƒ lÃ  claim)
    - CÃ³ sá»‘ liá»‡u
    - CÃ³ tÃªn riÃªng
    - CÃ³ ngÃ y thÃ¡ng
    """
    # CÃ³ sá»‘
    if re.search(r'\d+', text):
        return True
    
    # CÃ³ tÃªn riÃªng (chá»¯ hoa á»Ÿ giá»¯a cÃ¢u)
    words = text.split()
    for i, word in enumerate(words):
        if i > 0 and word and word[0].isupper():
            # KhÃ´ng pháº£i Ä‘áº§u cÃ¢u vÃ  viáº¿t hoa
            return True
    
    return False

def is_likely_non_claim(text):
    """
    XÃ¡c Ä‘á»‹nh cÃ¢u cÃ³ kháº£ nÄƒng lÃ  non-claim
    Returns: (is_non_claim, reason)
    """
    text = text.strip()
    
    # CÃ¢u quÃ¡ ngáº¯n
    word_count = len(text.split())
    if word_count < 5:
        return True, 'too_short'
    
    # CÃ¢u quÃ¡ dÃ i
    if word_count > 60:
        return True, 'too_long'
    
    # CÃ¢u há»i
    if is_question(text):
        return True, 'question'
    
    # Ã kiáº¿n
    if is_opinion(text):
        return True, 'opinion'
    
    # Má»‡nh lá»‡nh
    if is_command(text):
        return True, 'command'
    
    # CÃ¢u ná»‘i ngáº¯n
    if is_connector(text) and word_count < 15:
        return True, 'connector'
    
    return False, None

def extract_non_claims_from_context(row, max_per_context=3):
    """
    TrÃ­ch xuáº¥t non-claims tá»« context
    Chá»‰ láº¥y cÃ¡c cÃ¢u RÃ• RÃ€NG khÃ´ng pháº£i claim
    """
    context = row['context']
    claim = row['claim']
    evidence = row.get('evidence', '') or ''
    
    non_claims = []
    
    # TÃ¡ch cÃ¢u
    sentences = split_sentences(context)
    
    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue
        
        # Bá» qua náº¿u trÃ¹ng hoáº·c chá»©a claim
        if claim.lower() in sent.lower() or sent.lower() in claim.lower():
            continue
        
        # Bá» qua náº¿u trÃ¹ng hoáº·c chá»©a evidence
        if evidence and (evidence.lower() in sent.lower() or sent.lower() in evidence.lower()):
            continue
        
        # Kiá»ƒm tra cÃ³ pháº£i non-claim khÃ´ng
        is_non, reason = is_likely_non_claim(sent)
        
        if is_non:
            non_claims.append({
                'text': sent,
                'label': 'non-claim',
                'source': f'context_{reason}',
                'domain': row['domain']
            })
        elif not has_specific_info(sent):
            # CÃ¢u khÃ´ng cÃ³ thÃ´ng tin cá»¥ thá»ƒ â†’ cÃ³ thá»ƒ lÃ  non-claim
            non_claims.append({
                'text': sent,
                'label': 'non-claim',
                'source': 'context_general',
                'domain': row['domain']
            })
    
    # Giá»›i háº¡n sá»‘ lÆ°á»£ng
    if len(non_claims) > max_per_context:
        non_claims = non_claims[:max_per_context]
    
    return non_claims

def create_claims(df):
    """Táº¡o claims tá»« field 'claim'"""
    claims = []
    for _, row in df.iterrows():
        claims.append({
            'text': row['claim'],
            'label': 'claim',
            'source': 'claim_field',
            'domain': row['domain']
        })
    return claims

def create_template_non_claims():
    """
    Táº¡o non-claims tá»« templates
    DÃ¹ng Ä‘á»ƒ bá»• sung náº¿u khÃ´ng Ä‘á»§ non-claims tá»« context
    """
    non_claims = []
    
    # CÃ¢u há»i Ä‘a dáº¡ng
    questions = [
        "Báº¡n cÃ³ biáº¿t vá» Ä‘iá»u nÃ y khÃ´ng?",
        "Táº¡i sao láº¡i nhÆ° váº­y?",
        "Khi nÃ o sá»± kiá»‡n nÃ y diá»…n ra?",
        "Ai lÃ  ngÆ°á»i chá»‹u trÃ¡ch nhiá»‡m?",
        "LÃ m tháº¿ nÃ o Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á»?",
        "Äiá»u gÃ¬ sáº½ xáº£y ra tiáº¿p theo?",
        "CÃ³ pháº£i Ä‘Ã¢y lÃ  sá»± tháº­t khÃ´ng?",
        "ChÃºng ta nÃªn lÃ m gÃ¬ bÃ¢y giá»?",
        "Liá»‡u Ä‘iá»u nÃ y cÃ³ Ä‘Ãºng khÃ´ng?",
        "Báº¡n nghÄ© sao vá» váº¥n Ä‘á» nÃ y?",
        "CÃ³ ai biáº¿t thÃ´ng tin vá» viá»‡c nÃ y?",
        "Khi nÃ o chÃºng ta sáº½ cÃ³ cÃ¢u tráº£ lá»i?",
        "Táº¡i sao khÃ´ng cÃ³ ai nÃ³i vá» Ä‘iá»u nÃ y?",
        "LÃ m sao Ä‘á»ƒ xÃ¡c minh thÃ´ng tin?",
        "CÃ³ báº±ng chá»©ng nÃ o chá»©ng minh khÃ´ng?",
        "Ai Ä‘Ã£ Ä‘Æ°a ra tuyÃªn bá»‘ nÃ y?",
        "Nguá»“n thÃ´ng tin tá»« Ä‘Ã¢u?",
        "CÃ³ thá»ƒ tin tÆ°á»Ÿng Ä‘Æ°á»£c khÃ´ng?",
        "Äiá»u nÃ y cÃ³ áº£nh hÆ°á»Ÿng gÃ¬?",
        "ChÃºng ta cáº§n lÃ m gÃ¬ tiáº¿p theo?",
        "Váº¥n Ä‘á» nÃ y báº¯t Ä‘áº§u tá»« khi nÃ o?",
        "Ai lÃ  ngÆ°á»i Ä‘áº§u tiÃªn phÃ¡t hiá»‡n ra?",
        "CÃ³ giáº£i phÃ¡p nÃ o khÃ¡c khÃ´ng?",
        "TÃ¬nh hÃ¬nh hiá»‡n táº¡i nhÆ° tháº¿ nÃ o?",
        "CÃ³ ai pháº£n Ä‘á»‘i Ä‘iá»u nÃ y khÃ´ng?",
    ]
    
    # Ã kiáº¿n Ä‘a dáº¡ng
    opinions = [
        "TÃ´i nghÄ© ráº±ng Ä‘Ã¢y lÃ  má»™t quyáº¿t Ä‘á»‹nh Ä‘Ãºng Ä‘áº¯n",
        "Theo tÃ´i, váº¥n Ä‘á» nÃ y cáº§n Ä‘Æ°á»£c xem xÃ©t ká»¹ lÆ°á»¡ng hÆ¡n",
        "CÃ³ láº½ chÃºng ta nÃªn chá» Ä‘á»£i thÃªm thÃ´ng tin",
        "TÃ´i cáº£m tháº¥y Ä‘iá»u nÃ y khÃ´ng hoÃ n toÃ n chÃ­nh xÃ¡c",
        "DÆ°á»ng nhÆ° tÃ¬nh hÃ¬nh Ä‘ang cÃ³ nhá»¯ng chuyá»ƒn biáº¿n tÃ­ch cá»±c",
        "TÃ´i tin ráº±ng má»i thá»© sáº½ tá»‘t Ä‘áº¹p hÆ¡n",
        "CÃ³ váº» nhÆ° Ä‘Ã¢y lÃ  má»™t xu hÆ°á»›ng Ä‘Ã¡ng chÃº Ã½",
        "Theo quan Ä‘iá»ƒm cá»§a tÃ´i, Ä‘Ã¢y lÃ  váº¥n Ä‘á» quan trá»ng",
        "TÃ´i cho ráº±ng cáº§n cÃ³ thÃªm nghiÃªn cá»©u",
        "CÃ³ thá»ƒ nÃ³i ráº±ng Ä‘Ã¢y lÃ  má»™t bÆ°á»›c tiáº¿n lá»›n",
        "TÃ´i khÃ´ng cháº¯c cháº¯n vá» Ä‘iá»u nÃ y",
        "Theo Ã½ kiáº¿n cÃ¡ nhÃ¢n, Ä‘Ã¢y lÃ  váº¥n Ä‘á» phá»©c táº¡p",
        "TÃ´i nghÄ© chÃºng ta nÃªn tháº­n trá»ng",
        "CÃ³ láº½ Ä‘Ã¢y khÃ´ng pháº£i lÃ  giáº£i phÃ¡p tá»‘t nháº¥t",
        "TÃ´i cáº£m tháº¥y cáº§n thÃªm thá»i gian Ä‘á»ƒ Ä‘Ã¡nh giÃ¡",
        "DÆ°á»ng nhÆ° cÃ³ nhiá»u khÃ­a cáº¡nh cáº§n xem xÃ©t",
        "TÃ´i tin ráº±ng sáº½ cÃ³ cÃ¡ch giáº£i quyáº¿t tá»‘t hÆ¡n",
        "CÃ³ váº» nhÆ° tÃ¬nh hÃ¬nh Ä‘ang Ä‘Æ°á»£c cáº£i thiá»‡n",
        "Theo tÃ´i tháº¥y, Ä‘Ã¢y lÃ  hÆ°á»›ng Ä‘i Ä‘Ãºng Ä‘áº¯n",
        "TÃ´i cho ráº±ng cáº§n cÃ³ sá»± thay Ä‘á»•i",
        "CÃ¡ nhÃ¢n tÃ´i khÃ´ng Ä‘á»“ng Ã½ vá»›i quan Ä‘iá»ƒm nÃ y",
        "TÃ´i nghÄ© Ä‘Ã¢y chá»‰ lÃ  má»™t pháº§n cá»§a váº¥n Ä‘á»",
        "Theo tÃ´i hiá»ƒu, tÃ¬nh hÃ¬nh phá»©c táº¡p hÆ¡n nhiá»u",
        "TÃ´i cáº£m tháº¥y lo ngáº¡i vá» Ä‘iá»u nÃ y",
        "CÃ³ láº½ chÃºng ta Ä‘ang bá» qua Ä‘iá»u gÃ¬ Ä‘Ã³",
    ]
    
    # Má»‡nh lá»‡nh Ä‘a dáº¡ng
    commands = [
        "HÃ£y xem xÃ©t ká»¹ lÆ°á»¡ng váº¥n Ä‘á» nÃ y",
        "Äá»«ng quÃªn kiá»ƒm tra thÃ´ng tin trÆ°á»›c khi chia sáº»",
        "Cáº§n pháº£i cÃ³ thÃªm nghiÃªn cá»©u vá» chá»§ Ä‘á» nÃ y",
        "NÃªn tham kháº£o Ã½ kiáº¿n chuyÃªn gia trÆ°á»›c khi quyáº¿t Ä‘á»‹nh",
        "HÃ£y Ä‘á»c ká»¹ tÃ i liá»‡u trÆ°á»›c khi Ä‘Æ°a ra káº¿t luáº­n",
        "Äá»«ng tin vÃ o thÃ´ng tin chÆ°a Ä‘Æ°á»£c xÃ¡c minh",
        "Cáº§n kiá»ƒm tra nguá»“n gá»‘c cá»§a thÃ´ng tin",
        "HÃ£y suy nghÄ© tháº­t ká»¹ trÆ°á»›c khi hÃ nh Ä‘á»™ng",
        "Äá»«ng vá»™i vÃ ng Ä‘Æ°a ra nháº­n Ä‘á»‹nh",
        "HÃ£y tÃ¬m hiá»ƒu thÃªm vá» váº¥n Ä‘á» nÃ y",
        "Cáº§n pháº£i xÃ¡c minh tá»« nhiá»u nguá»“n",
        "NÃªn chá» Ä‘á»£i thÃªm thÃ´ng tin chÃ­nh thá»©c",
        "HÃ£y giá»¯ thÃ¡i Ä‘á»™ khÃ¡ch quan",
        "Äá»«ng lan truyá»n thÃ´ng tin sai lá»‡ch",
        "Cáº§n cÃ³ báº±ng chá»©ng cá»¥ thá»ƒ",
        "HÃ£y cÃ¢n nháº¯c táº¥t cáº£ cÃ¡c khÃ­a cáº¡nh",
        "Äá»«ng bá» qua nhá»¯ng chi tiáº¿t quan trá»ng",
        "Cáº§n láº¯ng nghe nhiá»u Ã½ kiáº¿n khÃ¡c nhau",
        "HÃ£y Ä‘áº·t cÃ¢u há»i trÆ°á»›c khi tin",
        "NÃªn so sÃ¡nh vá»›i cÃ¡c nguá»“n khÃ¡c",
    ]
    
    # CÃ¢u mÃ´ táº£ chung
    descriptions = [
        "ÄÃ¢y lÃ  má»™t váº¥n Ä‘á» phá»©c táº¡p cáº§n Ä‘Æ°á»£c xem xÃ©t tá»« nhiá»u gÃ³c Ä‘á»™",
        "TÃ¬nh hÃ¬nh hiá»‡n táº¡i Ä‘ang cÃ³ nhiá»u diá»…n biáº¿n khÃ¡c nhau",
        "Váº¥n Ä‘á» nÃ y Ä‘Ã£ thu hÃºt sá»± quan tÃ¢m cá»§a dÆ° luáº­n",
        "Nhiá»u ngÆ°á»i Ä‘ang tháº£o luáº­n vá» chá»§ Ä‘á» nÃ y",
        "Trong khi Ä‘Ã³, cÃ¡c chuyÃªn gia váº«n Ä‘ang tranh luáº­n",
        "BÃªn cáº¡nh Ä‘Ã³, cÃ²n cÃ³ nhiá»u yáº¿u tá»‘ cáº§n xem xÃ©t",
        "NgoÃ i ra, cáº§n chÃº Ã½ Ä‘áº¿n cÃ¡c khÃ­a cáº¡nh khÃ¡c",
        "Äá»“ng thá»i, váº¥n Ä‘á» nÃ y cÅ©ng liÃªn quan Ä‘áº¿n nhiá»u lÄ©nh vá»±c",
        "Tuy nhiÃªn, váº«n cÃ²n nhiá»u Ä‘iá»u chÆ°a rÃµ rÃ ng",
        "Máº·c dÃ¹ váº­y, cáº§n thÃªm thá»i gian Ä‘á»ƒ Ä‘Ã¡nh giÃ¡",
        "Do Ä‘Ã³, chÃºng ta cáº§n tháº­n trá»ng trong viá»‡c Ä‘Æ°a ra káº¿t luáº­n",
        "VÃ¬ váº­y, cáº§n cÃ³ thÃªm nghiÃªn cá»©u sÃ¢u hÆ¡n",
        "Theo Ä‘Ã³, tÃ¬nh hÃ¬nh Ä‘ang Ä‘Æ°á»£c theo dÃµi cháº·t cháº½",
        "NhÆ° váº­y, váº¥n Ä‘á» váº«n Ä‘ang Ä‘Æ°á»£c xem xÃ©t",
        "TÃ³m láº¡i, Ä‘Ã¢y lÃ  má»™t chá»§ Ä‘á» Ä‘Ã¡ng quan tÃ¢m",
        "NhÃ¬n chung, tÃ¬nh hÃ¬nh váº«n Ä‘ang diá»…n biáº¿n phá»©c táº¡p",
        "TrÃªn thá»±c táº¿, cÃ³ nhiá»u yáº¿u tá»‘ cáº§n cÃ¢n nháº¯c",
        "Vá» cÆ¡ báº£n, Ä‘Ã¢y lÃ  váº¥n Ä‘á» cáº§n Ä‘Æ°á»£c giáº£i quyáº¿t",
        "NÃ³i chung, má»i ngÆ°á»i Ä‘á»u quan tÃ¢m Ä‘áº¿n Ä‘iá»u nÃ y",
        "Cuá»‘i cÃ¹ng, chÃºng ta cáº§n chá» Ä‘á»£i thÃªm thÃ´ng tin",
    ]
    
    all_templates = questions + opinions + commands + descriptions
    
    for text in all_templates:
        non_claims.append({
            'text': text,
            'label': 'non-claim',
            'source': 'template',
            'domain': 'general'
        })
    
    return non_claims

def create_dataset(input_files, output_prefix):
    """
    Táº¡o dataset claim detection
    """
    print("=" * 70)
    print("Táº O DATASET CLAIM DETECTION")
    print("=" * 70)
    
    # Load data
    print("\n[1] Äang load dá»¯ liá»‡u...")
    all_data = []
    for file_path in input_files:
        try:
            df = load_ise_dataset(file_path)
            all_data.append(df)
            print(f"âœ“ Loaded {len(df)} samples")
        except Exception as e:
            print(f"âœ— Error: {e}")
    
    df_all = pd.concat(all_data, ignore_index=True)
    print(f"âœ“ Tá»•ng: {len(df_all)} samples")
    
    # Táº¡o claims
    print("\n[2] Táº¡o claims tá»« field 'claim'...")
    claims = create_claims(df_all)
    print(f"âœ“ {len(claims)} claims")
    
    # TrÃ­ch xuáº¥t non-claims tá»« context
    print("\n[3] TrÃ­ch xuáº¥t non-claims tá»« context...")
    context_non_claims = []
    for idx, row in df_all.iterrows():
        extracted = extract_non_claims_from_context(row, max_per_context=2)
        context_non_claims.extend(extracted)
        
        if (idx + 1) % 5000 == 0:
            print(f"  Processed {idx + 1}/{len(df_all)} samples...")
    
    print(f"âœ“ {len(context_non_claims)} non-claims tá»« context")
    
    # Thá»‘ng kÃª nguá»“n non-claims
    source_counts = {}
    for nc in context_non_claims:
        source = nc['source']
        source_counts[source] = source_counts.get(source, 0) + 1
    print("  PhÃ¢n bá»‘ nguá»“n:")
    for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):
        print(f"    - {source}: {count}")
    
    # Táº¡o template non-claims Ä‘á»ƒ bá»• sung
    print("\n[4] Táº¡o template non-claims...")
    template_non_claims = create_template_non_claims()
    print(f"âœ“ {len(template_non_claims)} templates")
    
    # Káº¿t há»£p non-claims
    all_non_claims = context_non_claims + template_non_claims
    print(f"âœ“ Tá»•ng non-claims: {len(all_non_claims)}")
    
    # CÃ¢n báº±ng dataset
    print("\n[5] CÃ¢n báº±ng dataset...")
    num_claims = len(claims)
    
    if len(all_non_claims) < num_claims:
        # Duplicate non-claims náº¿u cáº§n
        multiplier = (num_claims // len(all_non_claims)) + 1
        all_non_claims = all_non_claims * multiplier
    
    # Shuffle vÃ  láº¥y Ä‘á»§ sá»‘ lÆ°á»£ng
    np.random.seed(42)
    np.random.shuffle(all_non_claims)
    all_non_claims = all_non_claims[:num_claims]
    
    print(f"âœ“ Claims: {len(claims)}")
    print(f"âœ“ Non-claims: {len(all_non_claims)}")
    
    # Káº¿t há»£p
    all_samples = claims + all_non_claims
    df_final = pd.DataFrame(all_samples)
    df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"\n[6] Tá»•ng: {len(df_final)} samples")
    print("PhÃ¢n bá»‘ labels:")
    print(df_final['label'].value_counts())
    print("\nPhÃ¢n bá»‘ nguá»“n:")
    print(df_final['source'].value_counts())
    
    # Chia train/val/test
    print("\n[7] Chia train/val/test (70/15/15)...")
    train_val, test = train_test_split(
        df_final, test_size=0.15, random_state=42, stratify=df_final['label']
    )
    train, val = train_test_split(
        train_val, test_size=0.176, random_state=42, stratify=train_val['label']
    )
    
    print(f"âœ“ Train: {len(train)}")
    print(f"âœ“ Val: {len(val)}")
    print(f"âœ“ Test: {len(test)}")
    
    # LÆ°u files
    print("\n[8] LÆ°u files...")
    for df, name in [(train, 'train'), (val, 'val'), (test, 'test')]:
        file_path = f"{output_prefix}_{name}.jsonl"
        with open(file_path, 'w', encoding='utf-8') as f:
            for _, row in df.iterrows():
                json_obj = {'text': row['text'], 'label': row['label']}
                f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')
        print(f"âœ“ {file_path}")
    
    print("\n" + "=" * 70)
    print("âœ“ HOÃ€N THÃ€NH!")
    print("=" * 70)
    
    return train, val, test

if __name__ == "__main__":
    # Cáº¥u hÃ¬nh
    input_files = [
        "data/dataset/Dataset/ise-dsc01-train.json",
        "data/dataset/Dataset/ise-dsc01-train_ver2.json"
    ]
    
    output_prefix = "data/claim_detection/claim_detection"
    
    # Táº¡o dataset
    train, val, test = create_dataset(input_files, output_prefix)
    
    print("\nğŸ“ Files Ä‘Ã£ táº¡o:")
    print(f"  - {output_prefix}_train.jsonl")
    print(f"  - {output_prefix}_val.jsonl")
    print(f"  - {output_prefix}_test.jsonl")
    
    print("\nğŸ’¡ BÆ°á»›c tiáº¿p theo:")
    print("  1. Upload 3 files JSONL lÃªn Kaggle")
    print("  2. Cháº¡y finetune_phobert_kaggle.py")
